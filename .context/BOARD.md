# aOa Context Intelligence - Work Board

> **Updated**: 2025-12-23 | **Phase**: 1 - Redis Scoring Engine
> **Goal**: Transform aOa from search tool to predictive prefetch (90% accuracy)

---

## Confidence Legend

| Indicator | Meaning | Action |
|-----------|---------|--------|
| ðŸŸ¢ | Confident - clear path, similar to existing code | Proceed freely |
| ðŸŸ¡ | Uncertain - some unknowns, may need quick research | Try first, then research |
| ðŸ”´ | Lost - significant unknowns, needs research first | Research before starting |

| Research | Agent | When to Use |
|----------|-------|-------------|
| 131 | 1-3-1 Pattern | Problem decomposition, understanding behavior |
| GH | Growth Hacker | Architecture decisions, best practices |
| - | None | Straightforward implementation |

---

## Active

| # | Task | Expected Output | Solution Pattern | Status | C | R |
|---|------|-----------------|------------------|--------|---|---|
| P2-001 | Implement confidence calculation | Score 0.0-1.0 per file | Normalize composite score | Ready | ðŸŸ¡ | GH |

---

## Phase 1 - Redis Scoring Engine âœ… COMPLETE

**Success Criteria**: Files ranked by recency + frequency + tag affinity. `/rank` endpoint returns top 10.
**Result**: 6/6 rubrics pass, average latency 21ms

| # | Task | Expected Output | Solution Pattern | Status |
|---|------|-----------------|------------------|--------|
| P1-001 | Create ranking package | `/src/ranking/__init__.py`, `redis_client.py` | New directory, class wrapping redis-py | âœ… |
| P1-002 | Implement score operations | `zadd()`, `zincrby()`, `zrange()` wrappers | RedisClient class with sorted set methods | âœ… |
| P1-003 | Add recency scoring | Files scored by last-access time | Normalized exponential decay (1hr half-life) | âœ… |
| P1-004 | Add frequency scoring | Files scored by access count | `ZINCRBY frequency:files 1 <file>` | âœ… |
| P1-005 | Add tag affinity scoring | Files scored per tag | `ZADD tag:<tag> <score> <file>` | âœ… |
| P1-006 | Modify intent-capture.py | Write scores on every intent capture | POST to /rank/record on each file | âœ… |
| P1-007 | Implement composite scoring | Combined score from all signals | Weighted sum with normalization | âœ… |
| P1-008 | Add decay mechanism | Old scores fade over time | Exponential decay in scorer.py | âœ… |
| P1-009 | Add /rank endpoint | `GET /rank?tag=<tag>&limit=10` returns ranked files | New route in indexer.py | âœ… |
| P1-010 | Integration test | End-to-end: intent -> score -> rank | Benchmark rubrics (6/6 pass) | âœ… |

---

## Phase 2 - Predictive Prefetch (Week 2)

**Success Criteria**: PreHook outputs predicted files with confidence. `UserPromptSubmit` triggers predictions.

| # | Task | Expected Output | Solution Pattern | Deps | Status | C | R |
|---|------|-----------------|------------------|------|--------|---|---|
| P2-001 | Implement confidence calculation | Score 0.0-1.0 per file | Calibrated linear + evidence weighting | P1-007 | Active | ðŸŸ¢ | âœ“ |
| P2-002 | Create /predict endpoint | `GET /predict?tags=X,Y` returns predictions | New route using scorer | P2-001 | Queued | ðŸŸ¢ | - |
| P2-003 | Update intent-prefetch.py | Output predictions to Claude | JSON additionalContext format | P2-002 | Queued | ðŸŸ¢ | âœ“ |
| P2-004 | Implement context peek | First N lines of predicted files | Read + truncate in prefetch | P2-003 | Queued | ðŸŸ¢ | - |
| P2-005 | Add UserPromptSubmit hook | Predict on prompt submission | JSON additionalContext + keyword extraction | P2-003 | Queued | ðŸŸ¢ | âœ“ |
| P2-006 | Confidence threshold tuning | Only show predictions above X% | Start at 0.50, tune in P4 | P2-004 | Queued | ðŸŸ¢ | - |

---

## Phase 3 - Multi-Query Fusion (Week 3)

**Success Criteria**: `/context` endpoint returns ranked files + snippets for natural language intent.

| # | Task | Expected Output | Solution Pattern | Deps | Status | C | R |
|---|------|-----------------|------------------|------|--------|---|---|
| P3-001 | Create /context endpoint | `POST /context` with intent description | Combine /predict + snippet extraction | P2-004 | Queued | ðŸŸ¡ | GH |
| P3-002 | Add `aoa context` CLI | `aoa context "fix auth bug"` | CLI wrapper for /context | P3-001 | Queued | ðŸŸ¢ | - |
| P3-003 | Implement semantic matching | Map intent words to tags | Keyword extraction + tag lookup | P3-001 | Queued | ðŸ”´ | 131 |
| P3-004 | Bundle results with confidence | JSON with files, snippets, scores | Structured response format | P3-003 | Queued | ðŸŸ¢ | - |
| P3-005 | Caching layer | Cache common intents | Redis cache with TTL | P3-004 | Queued | ðŸŸ¡ | - |

---

## Phase 4 - Accuracy Tuning (Week 4)

**Success Criteria**: 90% of predicted files are actually used. Weights auto-tune based on feedback.

| # | Task | Expected Output | Solution Pattern | Deps | Status | C | R |
|---|------|-----------------|------------------|------|--------|---|---|
| P4-001 | Implement prediction logging | Log predictions vs actual usage | Store predictions, compare to intents | P2-003 | Queued | ðŸŸ¢ | - |
| P4-002 | Calculate hit rate metrics | % of predictions that were used | Compare predicted files to Read events | P4-001 | Queued | ðŸŸ¡ | - |
| P4-003 | Implement weight tuning | Adjust recency/frequency/tag weights | Gradient descent on hit rate | P4-002 | Queued | ðŸ”´ | GH |
| P4-004 | Add `aoa feedback` command | Manual feedback on predictions | `aoa feedback good/bad <file>` | P4-001 | Queued | ðŸŸ¢ | - |
| P4-005 | Dashboard metrics | View accuracy over time | `/metrics` endpoint or CLI | P4-002 | Queued | ðŸŸ¡ | - |
| P4-006 | Achieve 90% accuracy | Hit rate >= 90% | Iterate tuning until target met | P4-003 | Queued | ðŸ”´ | GH |

---

## Phases Overview

| Phase | Focus | Status | Blocked By | Success Metric |
|-------|-------|--------|------------|----------------|
| 1 | Redis Scoring Engine | âœ… Complete | - | /rank returns ranked files (6/6 rubrics) |
| 2 | Predictive Prefetch | Ready | - | PreHook outputs predictions |
| 3 | Multi-Query Fusion | Queued | Phase 2 | /context endpoint works |
| 4 | Accuracy Tuning | Queued | Phase 3 | 90% hit rate |

---

## Completed

| # | Task | Output | Completed |
|---|------|--------|-----------|
| P1 | Phase 1 - Redis Scoring Engine | 6/6 rubrics pass, 21ms avg latency | 2025-12-23 |
| - | GH Analysis | 4-phase implementation plan | 2025-12-23 |
| - | Current state assessment | Services running, 75 intents captured | 2025-12-23 |

---

## Architecture Notes

```
Current:
  intent-capture.py -> POST /intent -> indexer.py -> SQLite

Phase 1 adds:
  intent-capture.py -> POST /intent -> indexer.py -> SQLite
                                                  -> Redis (scores)

Phase 2 adds:
  intent-prefetch.py -> GET /predict -> scorer.py -> Redis
                     <- ranked files with confidence

Phase 3 adds:
  aoa context "..." -> POST /context -> semantic matcher
                    <- files + snippets + confidence
```

## Key Dependencies

- Redis: Already running in docker-compose
- redis-py: Need to verify installed
- Lua scripting: For atomic decay operations

---

## Confidence Assessment Rationale

### ðŸŸ¢ (Confident) - Phase 2 fully researched
- Standard CRUD operations, package creation, endpoint patterns
- Direct parallels in existing codebase (indexer.py endpoints)
- Well-documented Redis operations
- **P2-001**: Calibrated linear + evidence weighting (see details/p2-001-confidence-research.md)
- **P2-003**: JSON additionalContext format documented (see details/p2-003-prehook-research.md)
- **P2-005**: UserPromptSubmit hook config exists (see details/p2-005-userpromptsubmit-research.md)

### ðŸŸ¡ (Uncertain) - 5 tasks remaining
- P3-001: /context endpoint integration
- P4-002: Hit rate calculation methodology
- P4-005: Dashboard metrics approach

### ðŸ”´ (Lost) - 3 tasks (future phases)
- **P3-003 (Semantic matching)**: NLP approach vs simple keyword extraction?
- **P4-003/P4-006 (Weight tuning)**: Gradient descent or simpler heuristic?
